{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGLYJuXmFBZI"
   },
   "source": [
    "# **Module 6:** Generative AI for Cyber Security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tunning a model for Cyber Security\n",
    "\n",
    "Fine-tuning a language model in the context of LLMs (Masked Language Models like BERT), refers to the process of taking a pre-trained language model and further training it on a smaller, task-specific dataset to adapt it to a specific downstream task. The idea is to leverage the knowledge learned during the initial pre-training on a large corpus and then fine-tune the model to perform well on a specific task of interest.\n",
    "\n",
    "1. **Pre-training on a Large Corpus**\n",
    "\n",
    "    Initially, the language model is pre-trained on a large and diverse dataset. During this phase, the model learns general language patterns, syntax, and contextual relationships between words.\n",
    "\n",
    "1. **Task-Specific Data**\n",
    "\n",
    "    After pre-training, the model is fine-tuned on a smaller dataset that is specific to the task you want the model to perform well on. This dataset is typically labeled and consists of examples relevant to the downstream task.\n",
    "\n",
    "1. **Architecture and Parameters**\n",
    "\n",
    "    The architecture of the model remains the same, but the parameters learned during pre-training are further adjusted based on the task-specific data. The fine-tuning process updates the weights of the model to make it more suited for the specific task.\n",
    "\n",
    "\n",
    "1. **Task-Specific Objective Function**\n",
    "\n",
    "    The objective function used during fine-tuning is tailored to the downstream task. For example, in classification tasks, the model might be fine-tuned using a cross-entropy loss function.\n",
    "\n",
    "1. **Learning Rate and Training Hyperparameters**\n",
    "\n",
    "    Fine-tuning often involves adjusting the learning rate and other hyperparameters to ensure effective training on the smaller dataset. This helps prevent overfitting and encourages the model to adapt to the specific task.\n",
    "   \n",
    "1. **Transfer of Knowledge**\n",
    "\n",
    "    The knowledge gained during pre-training, such as understanding of language structures and semantics, is transferred to the task-specific model. Fine-tuning allows the model to specialize without losing the general language understanding acquired during pre-training.\n",
    "\n",
    "![](https://www.labellerr.com/blog/content/images/2023/08/Fine-tune-example.png)\n",
    "\n",
    "Fine-tuning is particularly useful when you have a limited amount of task-specific data. By starting with a pre-trained model, you can benefit from the knowledge embedded in the model and fine-tune it to achieve good performance on your specific task, even with a smaller dataset. This approach has been successful in various natural language processing (NLP) tasks, ranging from sentiment analysis to named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: SecureBret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SecureBERT](https://arxiv.org/pdf/2204.02685) is a domain-specific language model to represent cybersecurity textual data which is trained on a large amount of in-domain text crawled from online resources.\n",
    "\n",
    "SecureBERT can be used as the base model for any downstream task including text classification, NER, Seq-to-Seq, QA, etc.\n",
    "- SecureBERT has demonstrated significantly higher performance in predicting masked words within the text when compared to existing models like RoBERTa (base and large), SciBERT, and SecBERT.\n",
    "- SecureBERT has also demonstrated promising performance in preserving general English language understanding (representation).\n",
    "\n",
    "![](https://user-images.githubusercontent.com/46252665/195998237-9bbed621-8002-4287-ac0d-19c4f603d919.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "#!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ehsanaghaei/SecureBERT were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ehsanaghaei/SecureBERT and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "model = RobertaModel.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "\n",
    "inputs = tokenizer(\"This is SecureBERT!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "model = transformers.RobertaForCausalLM.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "\n",
    "def predict_mask(sent, tokenizer, model, topk =10, print_results = True):\n",
    "    token_ids = tokenizer.encode(sent, return_tensors='pt')\n",
    "    masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "    masked_pos = [mask.item() for mask in masked_position]\n",
    "    words = []\n",
    "    with torch.no_grad():\n",
    "        output = model(token_ids)\n",
    "\n",
    "    last_hidden_state = output[0].squeeze()\n",
    "\n",
    "    list_of_list = []\n",
    "    for index, mask_index in enumerate(masked_pos):\n",
    "        mask_hidden_state = last_hidden_state[mask_index]\n",
    "        idx = torch.topk(mask_hidden_state, k=topk, dim=0)[1]\n",
    "        words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
    "        words = [w.replace(' ','') for w in words]\n",
    "        list_of_list.append(words)\n",
    "\n",
    "    return list_of_list\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import html\n",
    "\n",
    "def input_masked_sentence(input):\n",
    "    def escape_mask(text):\n",
    "        return text.replace('<mask>', '<&zwj;mask>')\n",
    "    display(HTML(f\"<b>Input:</b> {escape_mask(input)}\"))\n",
    "    for output in predict_mask(input, tokenizer, model):\n",
    "        display(HTML(f\"<b>SecureBert:</b> {' | '.join(output)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `<mask>` token is commonly used in the context of masked language models (LLMs) or masked language model pre-training. This approach is a type of unsupervised learning where a model is trained to predict missing or masked tokens in a sequence of text. The `<mask>` token is used to represent the positions in the input text where tokens are masked or hidden during training.\n",
    "\n",
    "Here's a general overview of how the `<mask>` token works in the context of LLMs:\n",
    "\n",
    "**Masking during Training:**\n",
    "\n",
    "During the pre-training phase, a certain percentage of tokens in the input text are randomly selected to be masked. These masked tokens are then replaced with the `<mask>` token.\n",
    "The model is trained to predict the original identity of the masked tokens based on the context provided by the surrounding tokens.\n",
    "Objective Function:\n",
    "\n",
    "The objective function during training is typically to maximize the likelihood of predicting the correct tokens at the masked positions.\n",
    "This training process helps the model learn contextual relationships and dependencies between words in a given language.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/46252665/195998153-f5682f7c-60a8-486d-b2c1-9ef5732c24ba.png)\n",
    "\n",
    "**Fine-Tuning and Downstream Tasks:**\n",
    "\n",
    "After pre-training, the model can be fine-tuned on specific downstream tasks (such as text classification, named entity recognition, etc.).\n",
    "The knowledge gained during pre-training helps the model perform well on a range of natural language processing (NLP) tasks.\n",
    "Prediction during Inference:\n",
    "\n",
    "During inference or when using the model for downstream tasks, the `<mask>` token can be used to predict missing tokens in a given sequence. For example, if you provide a sentence with some tokens replaced by `<mask>`, the model can predict the most likely tokens for those masked positions.\n",
    "Overall, the `<mask>` token is a key element in the training process of LLMs, enabling them to learn rich representations of language and perform well on a variety of NLP tasks. The most well-known model that uses this approach is BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> Adversaries may also compromise sites then include <&zwj;mask> content designed to collect website authentication cookies from visitors."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> malicious | JavaScript | phishing | iframe | dynamic | additional | downloadable | hostile | embedded | website"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"Adversaries may also compromise sites then include <mask> content designed to collect website authentication cookies from visitors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> One example of this is MS14-068, which targets <&zwj;mask> and can be used to forge Kerberos tickets using domain user permissions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> Kerberos | authentication | users | Windows | administrators | LDAP | PAM | Samba | NTLM | AD"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"One example of this is MS14-068, which targets <mask> and can be used to forge Kerberos tickets using domain user permissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> Paris is the <&zwj;mask> of France."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> capital | Republic | Government | province | name | city | government | language | Capital | Bank"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"Paris is the <mask> of France.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> Virus causes <&zwj;mask>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> DoS | crash | reboot | reboots | panic | crashes | corruption | DOS | vulnerability | XSS"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"Virus causes <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> Sending huge amount of packets through network leads to <&zwj;mask>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> DoS | congestion | crashes | crash | problems | DOS | vulnerability | failure | vulnerabilities | errors"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"Sending huge amount of packets through network leads to <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> A <&zwj;mask> injection occurs when an attacker inserts malicious code into a server"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> code | SQL | command | malicious | script | web | vulnerability | server | ql | SQL"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"A <mask> injection occurs when an attacker inserts malicious code into a server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLMs as Conversational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "converse = pipeline(\"conversational\", model=\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Conversational Language Model (also known as Casual Language Model) is a type of language model designed to generate human-like responses in a conversational context. These models aim to understand and generate text in a way that simulates natural conversation. The main differences between conversational language model and our previous masked model are:\n",
    "\n",
    "- **Context-Awareness**\n",
    "\n",
    "    Conversational language models are designed to be context-aware, meaning they consider the preceding dialogue to generate relevant and coherent responses. They often utilize context from the conversation history to understand the user's intent and provide more accurate replies.\n",
    "\n",
    "- **Sequential Processing**\n",
    "\n",
    "    Conversational models process input sequentially, considering the conversation history turn by turn. This sequential nature enables them to generate responses based on the evolving context.\n",
    "\n",
    "- **Long-Term Dependency**\n",
    "\n",
    "    Effective conversational models need to capture long-term dependencies in the conversation. They should remember important information and references from earlier turns to provide meaningful and contextually appropriate responses.\n",
    "  \n",
    "- **User Intent Recognition**\n",
    "\n",
    "    Understanding user intent is crucial in conversational language models. These models often incorporate intent recognition mechanisms to identify the user's goals, queries, or commands from the input text.\n",
    "\n",
    "- **Dynamic Context Handling**\n",
    "\n",
    "    Conversational models dynamically update their understanding of the conversation as new information is provided. This dynamic context handling allows them to adapt to changes in user queries or context shifts during the conversation.\n",
    "\n",
    "- **Ethical Considerations**\n",
    "\n",
    "    Conversational language models should be designed with ethical considerations in mind. This includes addressing biases, preventing the generation of inappropriate or harmful content, and ensuring responsible use of the technology.\n",
    "\n",
    "![](https://assets-global.website-files.com/6305e5d52c28356b4fe71bac/63f8cfaeb05eed305bbc24f4_Holistic-AI-Figure-1.png)\n",
    "\n",
    "*Language Modeling Approaches. (a) Masked Language Modeling predict hidden words in the sequence. (b) Causal Language Modeling, predict the next word in the sequence*\n",
    "\n",
    "Prominent examples of conversational language models include OpenAI's ChatGPT, which is a powerful language model capable of generating context-aware responses in a conversational setting. These models have found applications in virtual assistants, chatbots, customer support systems, and various other conversational interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtVUvneuyAeI",
    "outputId": "0273865a-847c-407a-bb01-b0576d2516b6"
   },
   "outputs": [],
   "source": [
    "conversations = []\n",
    "def ask_model(input):\n",
    "    return converse(Conversation(\"Under the context of network and cyber security. \" + input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_model(\"What is DDoS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_model(\"How can we prevent it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SCEkDfpOFPQX",
    "3P69z9kfupES",
    "eLikdx9Legm0",
    "eh-k8KdMhJ_Q",
    "Iiskwr3VmoL5",
    "5PElFLh5oYzU",
    "pnHSPBrDodIs"
   ],
   "name": "Web Attack Detection (ML IDS)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
