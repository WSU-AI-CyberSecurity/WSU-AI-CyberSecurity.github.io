{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGLYJuXmFBZI"
   },
   "source": [
    "# **Module 6:** Generative AI for Cyber Security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tunning a model for Cyber Security\n",
    "\n",
    "Fine-tuning a language model in the context of LLMs (Masked Language Models like BERT), refers to the process of taking a pre-trained language model and further training it on a smaller, task-specific dataset to adapt it to a specific downstream task. The idea is to leverage the knowledge learned during the initial pre-training on a large corpus and then fine-tune the model to perform well on a specific task of interest.\n",
    "\n",
    "1. **Pre-training on a Large Corpus**\n",
    "\n",
    "    Initially, the language model is pre-trained on a large and diverse dataset. During this phase, the model learns general language patterns, syntax, and contextual relationships between words.\n",
    "\n",
    "1. **Task-Specific Data**\n",
    "\n",
    "    After pre-training, the model is fine-tuned on a smaller dataset that is specific to the task you want the model to perform well on. This dataset is typically labeled and consists of examples relevant to the downstream task.\n",
    "\n",
    "1. **Architecture and Parameters**\n",
    "\n",
    "    The architecture of the model remains the same, but the parameters learned during pre-training are further adjusted based on the task-specific data. The fine-tuning process updates the weights of the model to make it more suited for the specific task.\n",
    "\n",
    "\n",
    "1. **Task-Specific Objective Function**\n",
    "\n",
    "    The objective function used during fine-tuning is tailored to the downstream task. For example, in classification tasks, the model might be fine-tuned using a cross-entropy loss function.\n",
    "\n",
    "1. **Learning Rate and Training Hyperparameters**\n",
    "\n",
    "    Fine-tuning often involves adjusting the learning rate and other hyperparameters to ensure effective training on the smaller dataset. This helps prevent overfitting and encourages the model to adapt to the specific task.\n",
    "   \n",
    "1. **Transfer of Knowledge**\n",
    "\n",
    "    The knowledge gained during pre-training, such as understanding of language structures and semantics, is transferred to the task-specific model. Fine-tuning allows the model to specialize without losing the general language understanding acquired during pre-training.\n",
    "\n",
    "![](https://www.labellerr.com/blog/content/images/2023/08/Fine-tune-example.png)\n",
    "\n",
    "Fine-tuning is particularly useful when you have a limited amount of task-specific data. By starting with a pre-trained model, you can benefit from the knowledge embedded in the model and fine-tune it to achieve good performance on your specific task, even with a smaller dataset. This approach has been successful in various natural language processing (NLP) tasks, ranging from sentiment analysis to named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: SecureBret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SecureBERT](https://arxiv.org/pdf/2204.02685) is a domain-specific language model to represent cybersecurity textual data which is trained on a large amount of in-domain text crawled from online resources.\n",
    "\n",
    "SecureBERT can be used as the base model for any downstream task including text classification, NER, Seq-to-Seq, QA, etc.\n",
    "- SecureBERT has demonstrated significantly higher performance in predicting masked words within the text when compared to existing models like RoBERTa (base and large), SciBERT, and SecBERT.\n",
    "- SecureBERT has also demonstrated promising performance in preserving general English language understanding (representation).\n",
    "\n",
    "![](https://user-images.githubusercontent.com/46252665/195998237-9bbed621-8002-4287-ac0d-19c4f603d919.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ehsanaghaei/SecureBERT were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ehsanaghaei/SecureBERT and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "model = RobertaModel.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "\n",
    "inputs = tokenizer(\"This is SecureBERT!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "model = transformers.RobertaForCausalLM.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "\n",
    "\n",
    "def predict_mask(sent, tokenizer, model, topk=10, print_results=True):\n",
    "    token_ids = tokenizer.encode(sent, return_tensors=\"pt\")\n",
    "    masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "    masked_pos = [mask.item() for mask in masked_position]\n",
    "    words = []\n",
    "    with torch.no_grad():\n",
    "        output = model(token_ids)\n",
    "\n",
    "    last_hidden_state = output[0].squeeze()\n",
    "\n",
    "    list_of_list = []\n",
    "    for index, mask_index in enumerate(masked_pos):\n",
    "        mask_hidden_state = last_hidden_state[mask_index]\n",
    "        idx = torch.topk(mask_hidden_state, k=topk, dim=0)[1]\n",
    "        words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
    "        words = [w.replace(\" \", \"\") for w in words]\n",
    "        list_of_list.append(words)\n",
    "\n",
    "    return list_of_list\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import html\n",
    "\n",
    "\n",
    "def input_masked_sentence(input):\n",
    "    def escape_mask(text):\n",
    "        return text.replace(\"<mask>\", \"<&zwj;mask>\")\n",
    "\n",
    "    display(HTML(f\"<b>Input:</b> {escape_mask(input)}\"))\n",
    "    for output in predict_mask(input, tokenizer, model):\n",
    "        display(HTML(f\"<b>SecureBert:</b> {' | '.join(output)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `<mask>` token is commonly used in the context of masked language models (LLMs) or masked language model pre-training. This approach is a type of unsupervised learning where a model is trained to predict missing or masked tokens in a sequence of text. The `<mask>` token is used to represent the positions in the input text where tokens are masked or hidden during training.\n",
    "\n",
    "Here's a general overview of how the `<mask>` token works in the context of LLMs:\n",
    "\n",
    "**Masking during Training:**\n",
    "\n",
    "During the pre-training phase, a certain percentage of tokens in the input text are randomly selected to be masked. These masked tokens are then replaced with the `<mask>` token.\n",
    "The model is trained to predict the original identity of the masked tokens based on the context provided by the surrounding tokens.\n",
    "Objective Function:\n",
    "\n",
    "The objective function during training is typically to maximize the likelihood of predicting the correct tokens at the masked positions.\n",
    "This training process helps the model learn contextual relationships and dependencies between words in a given language.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/46252665/195998153-f5682f7c-60a8-486d-b2c1-9ef5732c24ba.png)\n",
    "\n",
    "**Fine-Tuning and Downstream Tasks:**\n",
    "\n",
    "After pre-training, the model can be fine-tuned on specific downstream tasks (such as text classification, named entity recognition, etc.).\n",
    "The knowledge gained during pre-training helps the model perform well on a range of natural language processing (NLP) tasks.\n",
    "Prediction during Inference:\n",
    "\n",
    "During inference or when using the model for downstream tasks, the `<mask>` token can be used to predict missing tokens in a given sequence. For example, if you provide a sentence with some tokens replaced by `<mask>`, the model can predict the most likely tokens for those masked positions.\n",
    "Overall, the `<mask>` token is a key element in the training process of LLMs, enabling them to learn rich representations of language and perform well on a variety of NLP tasks. The most well-known model that uses this approach is BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> Adversaries may also compromise sites then include <&zwj;mask> content designed to collect website authentication cookies from visitors."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> malicious | JavaScript | phishing | iframe | dynamic | additional | downloadable | hostile | embedded | website"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\n",
    "    \"Adversaries may also compromise sites then include <mask> content designed to collect website authentication cookies from visitors.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> One example of this is MS14-068, which targets <&zwj;mask> and can be used to forge Kerberos tickets using domain user permissions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> Kerberos | authentication | users | Windows | administrators | LDAP | PAM | Samba | NTLM | AD"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\n",
    "    \"One example of this is MS14-068, which targets <mask> and can be used to forge Kerberos tickets using domain user permissions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> Paris is the <&zwj;mask> of France."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> capital | Republic | Government | province | name | city | government | language | Capital | Bank"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"Paris is the <mask> of France.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> Virus causes <&zwj;mask>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> DoS | crash | reboot | reboots | panic | crashes | corruption | DOS | vulnerability | XSS"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"Virus causes <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> Sending huge amount of packets through network leads to <&zwj;mask>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> DoS | congestion | crashes | crash | problems | DOS | vulnerability | failure | vulnerabilities | errors"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\"Sending huge amount of packets through network leads to <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Input:</b> A <&zwj;mask> injection occurs when an attacker inserts malicious code into a server"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>SecureBert:</b> code | SQL | command | malicious | script | web | vulnerability | server | ql | SQL"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_masked_sentence(\n",
    "    \"A <mask> injection occurs when an attacker inserts malicious code into a server\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLMs as Offensive Generative AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offensive Generative AI Model\n",
    "\n",
    "Offensive security is the practice of actively seeking out vulnerabilities in an organization's cybersecurity. It often involves using similar tactics as attackers and might include red teaming, penetration testing and vulnerability assessments. Offensive security can be shortened to \"OffSec.\"\n",
    "\n",
    "Offensive generative AI refers natural language processing (NLP) and machine learning techniques, that are designed or trained to produce content that is offensive, harmful, or inappropriate in some way. This content could include hate speech, derogatory language, violent or threatening messages, misinformation, or other forms of harmful content. Malicious AI can weaponising generative AI to improve the monetisation of their attacks, which will stem from a surge in ransomware attacks and phishing campaigns.\n",
    "\n",
    "![image.png](https://canalys-prod-public.s3.eu-west-1.amazonaws.com/client/static/uimages/7Xb9lz2jbGfvTH1TJcfoGuID7VNqyocPICtqhesEJqerayYVsmLORCMJb1BQNSKC.png)\n",
    "\n",
    "Such AI systems can pose significant ethical and societal challenges. They have the potential to spread harmful messages at scale, amplify existing biases and prejudices present in the training data, and contribute to the proliferation of toxic online environments. Offensive generative AI can be used maliciously by individuals or groups to harass others, manipulate public opinion, or undermine trust in information sources.\n",
    "\r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative model for cracking password\n",
    "\n",
    "Generative AI poses several dangers for password cracking:\n",
    "\n",
    "- Speed and Efficiency: Generative AI can significantly speed up the process of password cracking by generating and testing a vast number of possible passwords in a short amount of time. This can make it much more efficient for attackers to breach password-protected systems or accounts.\n",
    "\n",
    "- Sophisticated Attack Techniques: Generative AI can employ sophisticated techniques, such as neural networks or reinforcement learning, to generate passwords that are more likely to be successful in cracking password hashes or bypassing authentication mechanisms.\n",
    "\n",
    "- Adaptability: Generative AI models can adapt to different patterns and structures commonly found in passwords, such as common words, phrases, or character combinations. This adaptability makes them more effective at cracking passwords that may not be easily guessable or vulnerable to traditional brute-force methods.\n",
    "\n",
    "- Privacy Risks: If generative AI is used for password cracking, it could compromise the privacy and security of individuals or organizations by gaining unauthorized access to sensitive information stored behind password-protected systems or accounts.\n",
    "\n",
    "- Scale and Automation: Generative AI can be deployed at scale and automated, allowing attackers to launch large-scale password cracking attacks against multiple targets simultaneously, increasing the potential impact and severity of security breaches.imators.\r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassGPT\n",
    "\n",
    "\n",
    "PassGPT is an LLM trained on password leaks for password generation. PassGPT outperforms existing methods based **by guessing twice as many previously unseen passwords**. PassGPT also contains the concept of guided password generation, where they leverage PassGPT sampling procedure to generate passwords matching arbitrary constraints.\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/WSU-AI-CyberSecurity/WSU-AI-CyberSecurity.github.io/main/assets/passgpt.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtVUvneuyAeI",
    "outputId": "0273865a-847c-407a-bb01-b0576d2516b6"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import RobertaTokenizerFast\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\n",
    "    \"javirandor/passgpt-10characters\",\n",
    "    max_len=12,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    do_lower_case=False,\n",
    "    strip_accents=False,\n",
    "    mask_token=\"<mask>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    truncation_side=\"right\",\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"javirandor/passgpt-10characters\").eval()\n",
    "\n",
    "NUM_GENERATIONS = 1\n",
    "\n",
    "\n",
    "def generate_password(prefix, max_length=12):\n",
    "    start_token = tokenizer.encode(prefix)\n",
    "    start_token = start_token[0:-1]\n",
    "\n",
    "    # Generate passwords sampling from the beginning of password token\n",
    "    g = model.generate(\n",
    "        torch.tensor([start_token]),\n",
    "        # do_sample=True,\n",
    "        num_return_sequences=NUM_GENERATIONS,\n",
    "        max_length=max_length + 1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bad_words_ids=[[tokenizer.bos_token_id]],\n",
    "    )\n",
    "\n",
    "    # Remove start of sentence token\n",
    "    g = g[:, 1:]\n",
    "\n",
    "    decoded = tokenizer.batch_decode(g.tolist())\n",
    "    decoded_clean = [\n",
    "        i.split(\"</s>\")[0] for i in decoded\n",
    "    ]  # Get content before end of password token\n",
    "\n",
    "    # Print your sampled passwords!\n",
    "    return decoded_clean\n",
    "\n",
    "\n",
    "seedgen = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the behaviour of human's password creation\n",
    "\n",
    "The following are generating the most likely password that people choose based on big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0876060007']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_password(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['password']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_password(prefix=\"pas\", max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwerty']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_password(prefix=\"qw\", max_length=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iloveme123']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_password(prefix=\"ilo\", max_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Johnny1994']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_password(prefix=\"John\", max_length=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also do a conditional password generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Map each of the desired character groups into their corresponding ids (as given by the tokenizer)\n",
    "lowercase = list(string.ascii_lowercase)\n",
    "uppercase = list(string.ascii_uppercase)\n",
    "digits = list(string.digits)\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "lowercase_tokens = tokenizer(lowercase, add_special_tokens=False).input_ids\n",
    "uppercase_tokens = tokenizer(uppercase, add_special_tokens=False).input_ids\n",
    "digits_tokens = tokenizer(digits, add_special_tokens=False).input_ids\n",
    "punctuation_tokens = tokenizer(punctuation, add_special_tokens=False).input_ids\n",
    "\n",
    "\n",
    "# All possible tokens in our model\n",
    "all_tokens = [[i] for i in range(len(tokenizer))]\n",
    "\n",
    "\n",
    "def conditional_generation(template, num_generations=1):\n",
    "    generated = 0\n",
    "    generations = []\n",
    "\n",
    "    while generated < num_generations:\n",
    "        generation = torch.tensor([tokenizer.bos_token_id]).unsqueeze(0)\n",
    "        current_length = 1\n",
    "\n",
    "        for char in template:\n",
    "            if char == \"l\":\n",
    "                bad_tokens = [i for i in all_tokens if i not in lowercase_tokens]\n",
    "            elif char == \"u\":\n",
    "                bad_tokens = [i for i in all_tokens if i not in uppercase_tokens]\n",
    "            elif char == \"d\":\n",
    "                bad_tokens = [i for i in all_tokens if i not in digits_tokens]\n",
    "            elif char == \"p\":\n",
    "                bad_tokens = [i for i in all_tokens if i not in punctuation_tokens]\n",
    "            else:\n",
    "                bad_tokens = [[tokenizer.eos_token_id]]\n",
    "\n",
    "            generation = model.generate(\n",
    "                generation.to(DEVICE),\n",
    "                do_sample=True,\n",
    "                max_length=current_length + 1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_return_sequences=1,\n",
    "                bad_words_ids=bad_tokens,\n",
    "            )\n",
    "            current_length += 1\n",
    "\n",
    "        if not 2 in generation.flatten():\n",
    "            generations.append(generation)\n",
    "            generated += 1\n",
    "\n",
    "    return tokenizer.batch_decode(torch.cat(generations, 0)[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Password Generation\n",
    "One of the main advantages of Generative AI is the possibility of generating passwords under arbitrary constraints. In this template code, we have created five different groups of characters that we can sample from at each position:\n",
    "\n",
    "- `l`: lowercase letters\n",
    "- `u`: uppercase letters\n",
    "- `d`: digits\n",
    "- `p`: punctuation\n",
    "- `*`: any character in the vocabulary\n",
    "- \n",
    "You can create any template by combining these. For example, `lllldd` will generate passwords starting with four lowercase letters and finishing with two digits.\n",
    "\n",
    "Feel free to create your own character groups below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['owen32', 'will14', 'bubb12', 'rgdl15', 'pinc04']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_generation(\"lllldd\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PSYCHOfr12', 'Lanenabb12', 'Pochacho99', 'SHADOWlu69', 'JayDanki07']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_generation(\"u*****lldd\", 5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SCEkDfpOFPQX",
    "3P69z9kfupES",
    "eLikdx9Legm0",
    "eh-k8KdMhJ_Q",
    "Iiskwr3VmoL5",
    "5PElFLh5oYzU",
    "pnHSPBrDodIs"
   ],
   "name": "Web Attack Detection (ML IDS)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
